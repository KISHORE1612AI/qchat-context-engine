#!/usr/bin/env bash
set -e

echo "ğŸš€ Starting Q Chat Context Engine..."
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

export ANONYMIZED_TELEMETRY=False
export CHROMA_TELEMETRY=False

# Start Ollama in background
ollama serve &

# Wait until Ollama API is ready
until curl -s http://localhost:11434/api/tags >/dev/null 2>&1; do
  echo "âŒ› Waiting for Ollama..."
  sleep 1
done
echo "âœ… Ollama is running"

# Ensure model exists
if ! ollama list | grep -q "embeddinggemma"; then
  echo "ğŸ“¦ Pulling embeddinggemma model..."
  ollama pull embeddinggemma
fi
echo "âœ… embeddinggemma ready"

# Ensure template exists
test -f templates/index.html || { echo "âŒ templates/index.html not found"; exit 1; }

# Install requirements (safe for cloud build)


echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "âœ… Starting FastAPI server..."
echo "ğŸŒ Listening on port ${PORT:-8080}"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# Start FastAPI using uvicorn
exec uvicorn backend:app --host 0.0.0.0 --port ${PORT:-8080}
